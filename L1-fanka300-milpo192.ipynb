{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students: Fanny Karelius (fanka300), Milda Poceviciute (milpo192)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we used urllib package to fetch the HTMLs from google play app store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "html_page = urllib.request.urlopen(\"https://play.google.com/store/apps/top\")\n",
    "soup = BeautifulSoup(html_page,\"lxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the HTML of the main page we found the hrefs of the different categories of the apps. We are going to use them for crawling the data on 1000 apps from different categories. In total we found 61 unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hrefs = list()\n",
    "for link in soup.findAll('a'):\n",
    "    hrefs.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_hrefs = list()\n",
    "for href in hrefs:\n",
    "    #print(href[0:20])\n",
    "    if href[0:20] ==  '/store/apps/category':\n",
    "        final_hrefs.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "final_hrefs = list(set(final_hrefs))\n",
    "print(len(final_hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "#x = urllib.request.urlopen('https://play.google.com/store/apps/category/GAME?hl=en').read().decode('utf-8')\n",
    "#print(x)\n",
    "final_list = list()\n",
    "for item in final_hrefs[0:55]:\n",
    "    url = 'https://play.google.com' + item\n",
    "    #print(url)\n",
    "    x = urllib.request.urlopen(url).read().decode('utf-8')    \n",
    "    appreg = r'href=\\\"(/store/apps/details.*?)\\\"'\n",
    "    appre = re.compile(appreg)\n",
    "    app_url_list = re.findall(appre,x)\n",
    "    app_url_list = list(set(app_url_list))\n",
    "    final_list.append(app_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching the HTMLs from the different categories, we end up having 1179 distinctive apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "apps = reduce(lambda x,y: x+ y, final_list, [])\n",
    "len(apps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the details (descriptions) of each of the apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_list = list()\n",
    "for app in apps:\n",
    "    #print(app)\n",
    "    url = 'https://play.google.com' + app + '&hl=en'\n",
    "    #print(url)\n",
    "    x = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    appreg = r'itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">.*?</div>'\n",
    "    appre = re.compile(appreg)\n",
    "    app_url_list = re.findall(appre,x)\n",
    "    app_url_list = list(set(app_url_list))\n",
    "    desc_list.append(app_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_final = list()\n",
    "for item in desc_list:\n",
    "    str1 = ''.join(item)\n",
    "#print(str1)\n",
    "    soup = BeautifulSoup(str1,\"lxml\")\n",
    "    desc_final.append(soup.div.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this section we have a raw data of the apps descriptions that we found at the google play website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we split the descriptions into lowercase words, remove non-alpha-numeric symbols and stop words, and finally used stemming to keep just the stems of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into words\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b')\n",
    "word_list = [tokenizer.tokenize(w) for w in desc_final]\n",
    "# make everything lowercase\n",
    "word_list = [[w.lower() for w in line] for line in word_list]\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "filtered_words = [[word for word in word_list_item if word not in stopwords.words('english')] for word_list_item in word_list]\n",
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the words\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [[ps.stem(word) for word in word_list_item] for word_list_item in word_list]\n",
    "#stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the documents into seperate strings in one list\n",
    "documents = [' '.join(x) for x in stemmed_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sklearn package to calculate the weights for each word: tf-idf weights. This represents how common each word is in each document. From the dimensions of the resulting matrix, we can see tha there are 1179 documents, and in total they contain 13519 distinctive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179, 13519)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transvector = TfidfVectorizer()\n",
    "fitted_docs = transvector.fit(documents)\n",
    "tfidf1 = transvector.fit_transform(documents)\n",
    "\n",
    "print(tfidf1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Query Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrote a function that does the same pre-processing on the query words (sentences) as we did for the apps' descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'listen', 'music']\n",
      "['hero', 'control', 'dragon', 'run']\n"
     ]
    }
   ],
   "source": [
    "# search keywords\n",
    "def prep_query(words):\n",
    "    # split into words\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b')\n",
    "    words2 = tokenizer.tokenize(words)\n",
    "    # make everything lowercase\n",
    "    words2 = [w.lower() for w in words2]\n",
    "    words2 = [word for word in words2 if word not in stopwords.words('english')]\n",
    "    ps = PorterStemmer()\n",
    "    query_words =[ps.stem(word) for word in words2]\n",
    "    return(query_words)\n",
    "\n",
    "keywords1 = \"I like to listening to the music\"\n",
    "query1 = prep_query(keywords1)\n",
    "print(query1)\n",
    "\n",
    "keywords2 = \"The hero controls the dragon to run.\"\n",
    "query2 = prep_query(keywords2)\n",
    "print(query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the tf-idf weights of the query (we used the function TfidfVectorizer in combination with transform function). The TfidfVectorizer created the vocabulary based on the app descriptions (when we used it with the fit function above), and also the frequency table (when used with the transform function). We create the frequency table of our new words (the query) based on that vocabulary (tf idf of the query). Then, the cosine distance between the query and the documents' frequency tables is computed. The higher the similarity, the better match the document is supposed to be to our query. So we order the document list based on this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def query_search(query):\n",
    "    values = transvector.transform(query)\n",
    "    #print(values.shape)\n",
    "    similarity = cosine_similarity(values.toarray(),tfidf1.toarray())\n",
    "    #print(similarity.shape)\n",
    "    sumsim = similarity.sum(axis=0)\n",
    "    document_ranking = np.argsort(-sumsim)\n",
    "    return(document_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_query1 = query_search(query1)\n",
    "docs_query2 = query_search(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 492  561 1145 ...,  453  440 1178]\n",
      "[ 385 1080  528 ...,  441  424 1178]\n"
     ]
    }
   ],
   "source": [
    "print(docs_query1)\n",
    "print(docs_query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our search algorithm chose app \"youtube.music\" as the best match for a query \"I like to listening to the music\". It seems to be a really good fit as the description does repeat the words \"music\" and \"listen\" several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/store/apps/details?id=com.google.android.apps.youtube.music\n",
      "youtub music is a new music app that allow you to easili find what you re look for and discov new music get playlist and recommend serv to you base on your context tast and what s trend around you a new music stream servic from youtub thi is a complet reimagin music servic with offici releas from your favorit artist find the music you want easili find the album singl live perform cover and remix you re look for don t know a song s name just search the lyric or describ it discov new music get music recommend base on tast locat and time of day use the hotlist to keep up with what s trend uninterrupt listen with music premium listen ad free don t worri about your music stop when you lock your screen or use other app download your favorit or let us do it for you by enabl offlin mixtap get one free month of music premium to listen ad free offlin and with your screen lock then pay just a month exist youtub red or googl play music member and user of either servic who have alreadi receiv a day trial are not elig monthli charg auto renew for music premium membership outsid of trial period\n"
     ]
    }
   ],
   "source": [
    "# query 1 top match\n",
    "print(apps[docs_query1[0]])\n",
    "print(documents[docs_query1[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search algorithm chose app \"mattel.eahdragons\" as the best match for a query \"The hero controls the dragon to run.\". When we look through the description of this app, it seems to fit our query sentence pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/store/apps/details?id=com.mattel.eahdragons\n",
      "babi dragon have arriv at ever after high just in time for dragon game hatch play feed style and train your babi dragon for the game fill her happi meter and your dragon will bring you a fun new surpris visit your dragon companion everyday to collect enough stamp to unlock a free friend brushfir charm featur style decor style your dragon with cute accessori pattern and magic aura decor your dragon s camp with fun anim decor and furnitur play fun game more babi dragon need lot of love and attent feed and play with your babi dragon to keep her healthi and happi don t forget to play with their cute critter friend teach your babi dragon how to fli dure flight train and collect gem along the way but be care to avoid those wick cloud your dragon is truli magic and it s time to put her fire breath skill to the test with magic train target practic develop your dragon s memori and match similar item through memori train reach level and go river raft with your babi dragon don t forget to collect those gem earn gem and orb by play with your dragon to purchas enchant new outfit and accessori for your pet rest relax dragon need to rest sometim after an adventur day give her some time to nap and she ll thank you for it your dragon look thirsti let her sip some tea befor go on your next big adventur look like your babi dragon want to play fetch throw the ball around with her adventur send your dragon on differ adventur in the land of ever after and earn reward to help you level up faster o go to the dark forest to rescu appl white or simpli exploreo journey to the land of giant and collect magic beanso travel to the enchant forest and meet pixi or save darl charmingo set out to the mirror realm and meet darl charm fli by ani time as mani more adventur await pictur take cute pictur of your dragon and your beauti camp setupinspir by the dragon game season of the netflix seri ever after high welcom to ever after high babi dragon pleas note thi app is free to play but some item can also be purchas for real money you can disabl in app purchas through your devic set thi app allow user to creat and modifi photo these photo are local to the devic and never share with ani third parti or store on our server unless otherwis specifi\n"
     ]
    }
   ],
   "source": [
    "# query 2 top match\n",
    "print(apps[docs_query2[0]])\n",
    "print(documents[docs_query2[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
